# Export

```shell
optimum-cli export [format] --model [model path] --task [task] [output path]
```

## Arguments

| **Optional Argument**                             | **Giải thích Ý nghĩa**                                                                                                                                                                                                                                                    | **Giá trị/Chú thích**                                                                                                                                                                                                   |
|---------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--task TASK`                                     | Xác định tác vụ export cho model. Đây là task dùng để CLI lựa chọn logic export phù hợp với mục đích của model, ví dụ: `text-generation`, `fill-mask`, `causal-lm-with-past`,…                                                                        | Ví dụ: `text-generation`, `fill-mask`, `question-answering`, `causal-lm-with-past`. Nếu không chỉ định, CLI có thể tự suy ra task dựa trên model.                                                                  |
| `--framework {pt,tf}`                             | Chỉ định framework gốc của model (PyTorch hoặc TensorFlow). Điều này giúp Optimum biết cách load checkpoint và xử lý các trọng số của model trong quá trình export.                                                                                                  | Các giá trị hỗ trợ: `pt` cho PyTorch và `tf` cho TensorFlow. Nếu không được chỉ định, CLI tự động suy ra dựa trên checkpoint có sẵn.                                                                                    |
| `--trust-remote-code`                             | Cho phép thực thi mã tùy chỉnh từ repository remote. Nếu model cần mã độc đáo không nằm trong thư viện gốc, flag này giúp kích hoạt tính năng đó.                                                                                                                     | Đây là flag, không cần giá trị thêm (chỉ cần xuất hiện).                                                                                                                                                               |
| `--weight-format {fp32,fp16,int8,int4}`             | Xác định định dạng số của trọng số model sau khi export. Việc chuyển đổi định dạng giúp giảm kích thước file và tối ưu inference theo độ chính xác mong muốn.                                                                                                         | Các giá trị khả dụng: `fp32` (đầy đủ độ chính xác), `fp16` (bán độ chính xác), `int8`, hoặc `int4` khi tiến hành quantization.                                                                                        |
| `--library {transformers,diffusers,timm,sentence_transformers}` | Chỉ định thư viện nguồn gốc của model. Tham số này giúp CLI áp dụng logic export phù hợp với cấu trúc và kiến trúc của model từ các thư viện khác nhau.                                                                                                                     | Các giá trị khả dụng: `transformers`, `diffusers`, `timm`, `sentence_transformers`.                                                                                                                                   |
| `--cache_dir CACHE_DIR`                           | Định nghĩa đường dẫn thư mục cache để lưu trữ các file model, tokenizer, và các thành phần phụ trợ trong quá trình export và tải xuống.                                                                                                                                | Ví dụ: `--cache_dir ./my_cache`.                                                                                                                                                                                        |
| `--pad-token-id PAD_TOKEN_ID`                     | Chỉ định giá trị token id dùng cho padding nếu model yêu cầu độ dài chuỗi cố định, giúp đảm bảo các input có cùng kích thước.                                                                                                                                        | Giá trị truyền vào là số nguyên, chẳng hạn: `--pad-token-id 0`.                                                                                                                                                        |
| `--ratio RATIO`                                   | Tham số điều chỉnh (ratio) dùng trong các quy trình export hoặc quantization nhằm cân bằng giữa chất lượng và hiệu năng của model.                                                                                                                                   | Thường là giá trị số thực, ví dụ: `--ratio 0.8`. Giá trị này phụ thuộc vào chiến lược export/quantization của backend.                                                                                                  |
| `--sym`                                           | Kích hoạt chế độ sử dụng symbolic shape khi export. Điều này hữu ích khi model có các kích thước input động, cho phép chia sẻ biểu thức đại số thay vì số cứng.                                                                                                      | Đây là flag; chỉ cần xuất hiện (không cần giá trị) để bật chế độ symbolic.                                                                                                                                             |
| `--group-size GROUP_SIZE`                         | Xác định kích thước của nhóm (grouping size) trong quá trình quantization theo nhóm. Điều này giúp cân bằng hiệu năng và chất lượng model sau quantization.                                                                                                      | Giá trị là số nguyên, ví dụ: `--group-size 128`.                                                                                                                                                                      |
| `--dataset DATASET`                               | Chỉ định dataset dùng trong quá trình calibration (hiệu chuẩn) của model, ví dụ như khi thực hiện quantization.                                                                                                                                                   | Có thể là tên dataset hoặc đường dẫn tới dữ liệu đảm bảo calibration chính xác cho model.                                                                                                                              |
| `--all-layers`                                    | Flag dùng để export toàn bộ các layer của model thay vì chỉ xuất ra những layer được tối ưu hóa cho inference. Điều này hữu ích khi bạn muốn chuyển đổi toàn bộ kiến trúc của model.                                                                               | Đây là một flag; xuất hiện flag này sẽ buộc export toàn bộ layer.                                                                                                                                                       |
| `--awq`                                           | Kích hoạt phương pháp Automated Weight Quantization (AWQ) trong quá trình export, nhằm tối ưu hóa trọng số model cho hiệu năng inference cao hơn.                                                                                                                  | Đây là flag; nếu xuất hiện, CLI sẽ áp dụng AWQ nếu backend hỗ trợ.                                                                                                                                                     |
| `--scale-estimation`                              | Kích hoạt quy trình tự ước lượng scale cho trọng số của model trong quá trình quantization, giúp điều chỉnh tỷ lệ cho phù hợp nhằm đạt chất lượng tốt nhất.                                                                                                     | Đây là flag; không yêu cầu giá trị đi kèm, chỉ cần bật để tiến hành quá trình scale estimation.                                                                                                                       |
| `--sensitivity-metric SENSITIVITY_METRIC`         | Chỉ định metric dùng để đo lường độ nhạy (ví dụ: sai số trung bình bình phương - MSE) trong quá trình quantization. Metric này giúp đánh giá ảnh hưởng của quantization lên hiệu năng của model.                                                            | Ví dụ: `--sensitivity-metric mse`. Giá trị có thể là tên của metric tùy thuộc vào backend.                                                                                                                          |
| `--num-samples NUM_SAMPLES`                       | Xác định số lượng mẫu dữ liệu được sử dụng trong quá trình calibration hoặc testing khi export. Việc này ảnh hưởng đến chất lượng hiệu chuẩn của quantization.                                                                                              | Giá trị là số nguyên, ví dụ: `--num-samples 100`.                                                                                                                                                                      |
| `--disable-stateful`                              | Tắt chế độ stateful khi export, tức là không giữ trạng thái giữa các bước inference. Điều này có thể hữu ích nếu bạn không cần sử dụng caching trong model.                                                                                                        | Đây là flag; nếu bật, quá trình export sẽ bỏ qua các tối ưu liên quan đến trạng thái (stateful).                                                                                                                         |
| `--disable-convert-tokenizer`                     | Tắt quá trình chuyển đổi (convert) tokenizer kèm theo model. Điều này có thể giúp tiết kiệm thời gian nếu bạn đã có tokenizer ở định dạng mong muốn và không cần chuyển đổi thêm.                                                                             | Đây là flag; chỉ cần xuất hiện để tắt bước chuyển đổi tokenizer.                                                                                                                                                      |

### Task

| **Task**                   | **Mô tả**                                                                                                                                                                                                 | **Ví dụ Model**                                                 |
|----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------|
| **default**                | Tác vụ mặc định, dùng làm fallback khi không có task chuyên biệt được định nghĩa. Nó áp dụng cho các model không rơi vào nhóm cụ thể nào khác.                                                       | Áp dụng chung cho model mà không nhấn mạnh một tác vụ cụ thể.       |
| **fill-mask**              | Dành cho các model thực hiện nhiệm vụ ẩn từ (masked language modeling), giúp dự đoán token bị ẩn trong câu.                                                                                              | BERT, RoBERTa                                                  |
| **text-classification**    | Dành cho bài toán phân loại văn bản, gán nhãn hoặc chú thích cho toàn bộ đoạn văn.                                                                                                                       | BERT hoặc DistilBERT được fine-tuned cho sentiment analysis      |
| **multiple-choice**        | Dành cho các tác vụ trả lời câu hỏi dạng lựa chọn, chọn đáp án đúng trong số các lựa chọn có sẵn.                                                                                                          | Model được fine-tuned trên các bài thi trắc nghiệm                |
| **token-classification**   | Dành cho các nhiệm vụ gán nhãn cho từng token trong câu (ví dụ: Named Entity Recognition, POS Tagging).                                                                                                  | BERT, RoBERTa được fine-tuned cho NER                              |
| **question-answering**     | Dành cho tác vụ trích xuất câu trả lời từ đoạn văn, dựa trên một câu hỏi cho trước.                                                                                                                       | BERT fine-tuned trên SQuAD                                        |
| **text-generation**        | Dành cho bài toán sinh văn bản tự động theo kiểu auto-regressive (không sử dụng caching).                                                                                                                 | Một số phiên bản của GPT‑2, GPT‑Neo (phiên bản không chuẩn bị cached state) |
| **causal-lm</br>causal-lm-with-past**    | Dành cho các model causal LM có cơ chế caching (past key values) để tăng tốc inference; task này tối ưu cho những model cần sinh văn bản dựa vào trạng thái ẩn của các bước trước đó.                 | GPT‑2, LLaMA, GPT‑3 (với cơ chế caching)                           |
| **feature-extraction**     | Dùng cho việc trích xuất các đặc trưng (representations) từ một mô hình, thường là từ các layer ẩn; không cần sử dụng đầu ra cuối cùng (logits) của model.                                          | BERT, RoBERTa (sử dụng đầu ra từ hidden layers)                    |
| **feature-extraction-with-past** | Phiên bản đặc biệt của feature-extraction dành cho các model có hỗ trợ caching, giúp tăng tốc trích xuất đặc trưng từ các bước tính toán đã được lưu trữ.                                             | Ít phổ biến, nhưng có thể áp dụng cho một số model causal LM có caching |

> **Lưu ý:**  
> - Tùy theo backend export (như onnx, openvino, …) và kiến trúc của model, danh sách task có thể khác nhau.  
> - Với model sinh văn bản causal LM, nếu hỗ trợ caching thì task **causal-lm-with-past** được ưu tiên; nếu không, người dùng có thể sử dụng **text-generation**.

## Export safetensors to onnx

```shell
optimum-cli export onnx --model [model path] --task [task] [output path]
```

## Export safetensors to openvino

```shell
optimum-cli export openvino --model [model path] --task [task] --weight-format int4 [output path]
```
